# 向量模型 (Embedding Model) 与 RAG 原理

`.env` 配置文件中的 `EMBEDDING_MODEL` 参数是构建**检索增强生成 (RAG)** 系统的核心基石。本文阐述其作用机制、核心价值及工程选型标准。

## 1. 核心定义

**Embedding (向量化)** 是将人类可读的非结构化数据（文本、图片、音频）转化为计算机可计算的**高维数值向量 (Vector)** 的过程。

- **输入**: "苹果"
- **输出**: `[0.12, -0.5, 0.88, ...]` (假设为 1536 维数组)

在这个高维空间中，**语义相近**的词，其向量坐标距离也越近。例如，"苹果"与"水果"的距离，远小于"苹果"与"卡车"的距离。

## 2. 核心价值：突破 Context Window 限制

Embedding 是解决 LLM **上下文长度限制 (Context Window)** 与 **Token 成本** 矛盾的关键技术。

### 痛点

- **容量瓶颈**: 即使是支持 128k Context 的模型，也无法一次性读入企业级海量知识库（如数百万字的文档）。
- **成本陷阱**: 若每次对话都将全量背景知识放入 Prompt，Token 消耗将呈指数级增长，极其昂贵且低效。

### 解决方案：RAG (检索增强生成)

Embedding 实际上实现了一种**按需加载**的记忆机制：

1.  **切分 (Chunking)**: 将海量文档切分为细小的语义片段。
2.  **索引 (Indexing)**: 利用 Embedding Model 将这些片段转化为向量，存入向量数据库 (Vector DB)。
3.  **检索 (Retrieval)**: 当用户提问时，将问题也转化为向量，在数据库中计算相似度（如余弦相似度），**只捞出**最相关的 Top-K 片段。
4.  **生成 (Generation)**: 将检索到的片段作为"临时知识"注入 Prompt，让 LLM 基于这些信息回答。

**本质**: AI 并非"读完"了所有书，而是在每次回答前，被临时"灌输"了那一瞬间最需要的几百字知识。

## 3. 启用时机 (工程经验)

在实际工程中，是否引入向量检索系统通常取决于数据规模：

| 场景             | 数据规模                  | 推荐方案            | 理由                                                                 |
| :--------------- | :------------------------ | :------------------ | :------------------------------------------------------------------- |
| **Full Context** | < 8k Tokens (约 1 万汉字) | **直接放入 Prompt** | 效果最好，AI 拥有"全知全能视野"且无检索损耗。                        |
| **RAG**          | > 10k ~ 海量              | **向量检索**        | 必须切片。不仅是容量问题，更是为了**抗噪**（避免无关信息干扰模型）。 |
| **实时/动态**    | 任意规模                  | **向量检索**        | 针对新闻、日志等高频更新数据，只有检索能保证时效性。                 |

## 4. 常见模型

- **OpenAI**: `text-embedding-3-small` / `text-embedding-3-large` (行业标准，性价比高)
- **Hugging Face**: `bge-m3`, `e5-large` (优秀的开源模型，支持本地部署)
- **Z.ai (智谱)**: `embedding-2` / `embedding-3` (中文语义理解较强)

---

_注：在本项目中，`EMBEDDING_MODEL` 主要用于 Runtime Lab Ch7 (Embedding 生成) 与 Ch8 (向量检索) 的实现。_
